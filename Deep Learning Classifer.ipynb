{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of bing search queries into geographic and non geographic entities. A sample of the data is prelabelled for training classifier\n",
    "\n",
    "### The notebook contains two parts:\n",
    "* ML classfiers using Logisitic regression, Naive Bayes, SVM and XGBoost\n",
    "* Deep learning Classifer using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_geo = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filtering geographical stopwords from the NLTK Corpus\n",
    "for i in ['as','at','by','between','to','from','in', 'off', 'there','where']:\n",
    "    stop_words_geo.remove(i)\n",
    "\n",
    "test_df_1 = pd.read_csv('./labelled_500.csv', usecols=['Query','geo'], nrows=499)\n",
    "test_df_2 = pd.read_csv('./labelled_1000_correct.csv', usecols=['Query','geo'], nrows=1500)\n",
    "test_df = test_df_1.append(test_df_2[499:1500])\n",
    "\n",
    "df_nongeo_sampled = test_df[test_df['geo'] == 0].sample(n=319, frac=None, \\\n",
    "                                replace=False, weights=None, random_state=None, axis=None)\n",
    "\n",
    "test_df_sampled = df_nongeo_sampled.append(test_df[test_df.geo == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting data into train - validation split)\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(test_df_sampled.Query.values, test_df_sampled.geo.values, \n",
    "                                                  stratify=test_df_sampled.geo.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying search queries using logistic regression, naivebayes, SVM and Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TfidfVectorizer \n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = stop_words_geo)\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765625\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "print metrics.accuracy_score(yvalid,np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.83\n",
      "Average recall score: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "average_precision = average_precision_score(yvalid, np.argmax(predictions, axis=1))\n",
    "average_recall_score = recall_score(yvalid, np.argmax(predictions, axis=1))\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "print('Average recall score: {0:0.2f}'.format(\n",
    "      average_recall_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CountVectorizer\n",
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = stop_words_geo)\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7265625\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "print metrics.accuracy_score(yvalid,np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.80\n",
      "Average recall score: 0.61\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "average_precision = average_precision_score(yvalid, np.argmax(predictions, axis=1))\n",
    "average_recall_score = recall_score(yvalid, np.argmax(predictions, axis=1))\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "print('Average recall score: {0:0.2f}'.format(\n",
    "      average_recall_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Naive Bayes using Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7421875\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "print metrics.accuracy_score(yvalid,np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.81\n",
      "Average recall score: 0.62\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "average_precision = average_precision_score(yvalid, np.argmax(predictions, axis=1))\n",
    "average_recall_score = recall_score(yvalid, np.argmax(predictions, axis=1))\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "print('Average recall score: {0:0.2f}'.format(\n",
    "      average_recall_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71875\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "print metrics.accuracy_score(yvalid,np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.79\n",
      "Average recall score: 0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "average_precision = average_precision_score(yvalid, np.argmax(predictions, axis=1))\n",
    "average_recall_score = recall_score(yvalid, np.argmax(predictions, axis=1))\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "print('Average recall score: {0:0.2f}'.format(\n",
    "      average_recall_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM using singular-value decompositionto preprocess the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734375\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple SVM\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "print metrics.accuracy_score(yvalid,np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.80\n",
      "Average recall score: 0.80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "average_precision = average_precision_score(yvalid, np.argmax(predictions, axis=1))\n",
    "average_recall_score = recall_score(yvalid, np.argmax(predictions, axis=1))\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "print('Average recall score: {0:0.2f}'.format(\n",
    "      average_recall_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  XGBoost for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7265625\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf svd features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print metrics.accuracy_score(yvalid,np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.80\n",
      "Average recall score: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "average_precision = average_precision_score(yvalid, np.argmax(predictions, axis=1))\n",
    "average_recall_score = recall_score(yvalid, np.argmax(predictions, axis=1))\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "print('Average recall score: {0:0.2f}'.format(\n",
    "      average_recall_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf svd features\n",
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "print metrics.accuracy_score(yvalid,np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.81\n",
      "Average recall score: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "average_precision = average_precision_score(yvalid, np.argmax(predictions, axis=1))\n",
    "average_recall_score = recall_score(yvalid, np.argmax(predictions, axis=1))\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "print('Average recall score: {0:0.2f}'.format(\n",
    "      average_recall_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(np.mean(yvalid == np.argmax(predictions, axis=1)), \\\n",
    "                                 greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [02:52, 12740.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words_geo]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:00<00:00, 1983.09it/s]\n",
      "100%|██████████| 128/128 [00:00<00:00, 2876.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8515625"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a xgboost on glove features\n",
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "np.mean(yvalid == np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.89\n",
      "Average recall score: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "average_precision = average_precision_score(yvalid, np.argmax(predictions, axis=1))\n",
    "average_recall_score = recall_score(yvalid, np.argmax(predictions, axis=1))\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "print('Average recall score: {0:0.2f}'.format(\n",
    "      average_recall_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8515625"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "np.mean(yvalid == np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.89\n",
      "Average recall score: 0.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "average_precision = average_precision_score(yvalid, np.argmax(predictions, axis=1))\n",
    "average_recall_score = recall_score(yvalid, np.argmax(predictions, axis=1))\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "print('Average recall score: {0:0.2f}'.format(\n",
    "      average_recall_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 3372.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 758.74it/s]\n"
     ]
    }
   ],
   "source": [
    "our_test = np.array(['cafe near google mountain view'])\n",
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xtest_glove = [sent2vec(x) for x in tqdm(our_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = clf.predict_proba(xtest_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deep Learning using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='softmax'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 510 samples, validate on 128 samples\n",
      "Epoch 1/5\n",
      "510/510 [==============================] - 1s 1ms/step - loss: 0.6128 - val_loss: 0.6850\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/5\n",
      "510/510 [==============================] - 0s 309us/step - loss: 0.4046 - val_loss: 0.6767\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/5\n",
      "510/510 [==============================] - 0s 263us/step - loss: 0.3132 - val_loss: 0.6691\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/5\n",
      "510/510 [==============================] - 0s 259us/step - loss: 0.2643 - val_loss: 0.6644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/5\n",
      "510/510 [==============================] - 0s 267us/step - loss: 0.2092 - val_loss: 0.6627\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13bd23d50>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=5, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using Keras tokenizer\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1507/1507 [00:00<00:00, 136501.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 510 samples, validate on 128 samples\n",
      "Epoch 1/100\n",
      "510/510 [==============================] - 8s 15ms/step - loss: 0.7093 - acc: 0.5157 - val_loss: 0.6624 - val_acc: 0.5234\n",
      "Epoch 2/100\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 0.6921 - acc: 0.5412 - val_loss: 0.6394 - val_acc: 0.5312\n",
      "Epoch 3/100\n",
      "510/510 [==============================] - 7s 13ms/step - loss: 0.6755 - acc: 0.5510 - val_loss: 0.6126 - val_acc: 0.6484\n",
      "Epoch 4/100\n",
      "510/510 [==============================] - 5s 11ms/step - loss: 0.6525 - acc: 0.6137 - val_loss: 0.5827 - val_acc: 0.7266\n",
      "Epoch 5/100\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.6222 - acc: 0.6627 - val_loss: 0.5484 - val_acc: 0.7656\n",
      "Epoch 6/100\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.5911 - acc: 0.6941 - val_loss: 0.5124 - val_acc: 0.7812\n",
      "Epoch 7/100\n",
      "510/510 [==============================] - 5s 11ms/step - loss: 0.5632 - acc: 0.7373 - val_loss: 0.4793 - val_acc: 0.8047\n",
      "Epoch 8/100\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.5359 - acc: 0.7373 - val_loss: 0.4536 - val_acc: 0.7969\n",
      "Epoch 9/100\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.4876 - acc: 0.7941 - val_loss: 0.4378 - val_acc: 0.8125\n",
      "Epoch 10/100\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.4655 - acc: 0.7902 - val_loss: 0.4326 - val_acc: 0.8203\n",
      "Epoch 11/100\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.4679 - acc: 0.7961 - val_loss: 0.4273 - val_acc: 0.8203\n",
      "Epoch 12/100\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.4212 - acc: 0.8275 - val_loss: 0.4145 - val_acc: 0.8281\n",
      "Epoch 13/100\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.3968 - acc: 0.8176 - val_loss: 0.3983 - val_acc: 0.8672\n",
      "Epoch 14/100\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.4206 - acc: 0.8294 - val_loss: 0.3974 - val_acc: 0.8750\n",
      "Epoch 15/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.4183 - acc: 0.8235 - val_loss: 0.4069 - val_acc: 0.8594\n",
      "Epoch 16/100\n",
      "510/510 [==============================] - 5s 11ms/step - loss: 0.4366 - acc: 0.8294 - val_loss: 0.4138 - val_acc: 0.8672\n",
      "Epoch 17/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.3892 - acc: 0.8529 - val_loss: 0.3945 - val_acc: 0.8750\n",
      "Epoch 18/100\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.3766 - acc: 0.8490 - val_loss: 0.3717 - val_acc: 0.8828\n",
      "Epoch 19/100\n",
      "510/510 [==============================] - 7s 13ms/step - loss: 0.3742 - acc: 0.8412 - val_loss: 0.3546 - val_acc: 0.8828\n",
      "Epoch 20/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.3558 - acc: 0.8412 - val_loss: 0.3458 - val_acc: 0.8750\n",
      "Epoch 21/100\n",
      "510/510 [==============================] - 7s 13ms/step - loss: 0.3556 - acc: 0.8373 - val_loss: 0.3427 - val_acc: 0.8750\n",
      "Epoch 22/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.3611 - acc: 0.8510 - val_loss: 0.3440 - val_acc: 0.8750\n",
      "Epoch 23/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.3390 - acc: 0.8569 - val_loss: 0.3510 - val_acc: 0.8828\n",
      "Epoch 24/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.3567 - acc: 0.8353 - val_loss: 0.3610 - val_acc: 0.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1240d9210>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy is 89 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.91\n",
      "Average recall score: 0.95\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(xvalid_pad)\n",
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "average_precision = average_precision_score(yvalid, np.argmax(predictions, axis=1))\n",
    "average_recall_score = recall_score(yvalid, np.argmax(predictions, axis=1))\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "print('Average recall score: {0:0.2f}'.format(\n",
    "      average_recall_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 510 samples, validate on 128 samples\n",
      "Epoch 1/100\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.7135 - acc: 0.5137 - val_loss: 0.6728 - val_acc: 0.5938\n",
      "Epoch 2/100\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.6845 - acc: 0.5412 - val_loss: 0.6471 - val_acc: 0.6016\n",
      "Epoch 3/100\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.6619 - acc: 0.6098 - val_loss: 0.6203 - val_acc: 0.6406\n",
      "Epoch 4/100\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.6520 - acc: 0.6118 - val_loss: 0.5897 - val_acc: 0.6875\n",
      "Epoch 5/100\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.6154 - acc: 0.6647 - val_loss: 0.5538 - val_acc: 0.7734\n",
      "Epoch 6/100\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.5935 - acc: 0.7118 - val_loss: 0.5135 - val_acc: 0.7891\n",
      "Epoch 7/100\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.5572 - acc: 0.7471 - val_loss: 0.4739 - val_acc: 0.7969\n",
      "Epoch 8/100\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.4955 - acc: 0.7745 - val_loss: 0.4371 - val_acc: 0.8203\n",
      "Epoch 9/100\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.4705 - acc: 0.7843 - val_loss: 0.4049 - val_acc: 0.8438\n",
      "Epoch 10/100\n",
      "510/510 [==============================] - 10s 19ms/step - loss: 0.4641 - acc: 0.7804 - val_loss: 0.4191 - val_acc: 0.8281\n",
      "Epoch 11/100\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.4087 - acc: 0.8235 - val_loss: 0.4404 - val_acc: 0.8359\n",
      "Epoch 12/100\n",
      "510/510 [==============================] - 10s 19ms/step - loss: 0.4508 - acc: 0.8235 - val_loss: 0.4212 - val_acc: 0.8438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13e8fe650>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.88\n",
      "Average recall score: 0.94\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(xvalid_pad)\n",
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "average_precision = average_precision_score(yvalid, np.argmax(predictions, axis=1))\n",
    "average_recall_score = recall_score(yvalid, np.argmax(predictions, axis=1))\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "print('Average recall score: {0:0.2f}'.format(\n",
    "      average_recall_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "510/510 [==============================] - 9s 18ms/step - loss: 0.7245 - acc: 0.4941\n",
      "Epoch 2/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.6889 - acc: 0.5471\n",
      "Epoch 3/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.6787 - acc: 0.5725\n",
      "Epoch 4/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.6470 - acc: 0.6039\n",
      "Epoch 5/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.6264 - acc: 0.6490\n",
      "Epoch 6/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.5900 - acc: 0.6765\n",
      "Epoch 7/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.5462 - acc: 0.7294\n",
      "Epoch 8/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.5113 - acc: 0.7490\n",
      "Epoch 9/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.4965 - acc: 0.7922\n",
      "Epoch 10/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.4798 - acc: 0.7765\n",
      "Epoch 11/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.4685 - acc: 0.7941\n",
      "Epoch 12/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.4714 - acc: 0.7863\n",
      "Epoch 13/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.4566 - acc: 0.7961\n",
      "Epoch 14/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.4237 - acc: 0.8333\n",
      "Epoch 15/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.4059 - acc: 0.8078\n",
      "Epoch 16/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.3891 - acc: 0.8275\n",
      "Epoch 17/100\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 0.3964 - acc: 0.8255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13d45aa10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='acc', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.90\n",
      "Average recall score: 0.94\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(xvalid_pad)\n",
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "average_precision = average_precision_score(yvalid, np.argmax(predictions, axis=1))\n",
    "average_recall_score = recall_score(yvalid, np.argmax(predictions, axis=1))\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "print('Average recall score: {0:0.2f}'.format(\n",
    "      average_recall_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As it can be seen, LSTM is providing the highest precision and recall combination on the particular dataset. If we focus on non neural net, boosting using xgboost provides the best precision, recall combination.\n",
    "\n",
    "### Additionally, the model will train better if trained on a larger labelled dataset hence after labelling the data the performance of the classifier will improve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
